\chapter{Ejercicio de Detección y seguimiento de caras con cámara PTZ}\label{cap.followface}
En este punto ya se han presentado contexto, los objetivos y las herramientas empleadas para llevar a cabo este proyecto. Este capítulo está reservado para detallar la realización de la primera de las dos prácticas creadas en y para el entorno JdeRobot. Se explicará la infraestructura desarrollada que le da soporte, el hardware involucrado y su forma de comunicación e interacción, el componente académico diseñado y la solución de referencia programada.

\section{Enunciado} \label{sec.enunciado}
El objetivo para el estudiante en esta práctica es que una cámara real sea capaz de seguir con su movimiento caras de personas. La cámara sólo proporcionará sus imágenes captadas, a través de una capturadora de vídeo que irá proporcionando al nodo académico los fotogramas sucesivos. Además, la cámara dispone de dos actuadores de movimiento que controlarán sus giros en horizontal y vertical, a los cuales a partir de ahora llamaremos \textit{Pan} y \textit{Tilt} respectivamente.

En la solución canónica el alumno deberá programar un algoritmo de segmentación de caras de personas en la imagen y un algoritmo de control gradual en base a los datos que proporciona la imagen segmentada. Para ello, hay disponibles distintos elementos de depuración en el interfaz gráfico (GUI), ya que incluye espacios para la visualización de la imagen captada por la cámara y de la imagen segmentada, accesibles con dos sencillos objetos Python.

El algoritmo canónico de control responde a un control reactivo, que en cada instante actuará en función de los datos de los sensores y sus propias variables internas. El control reactivo permitirá establecer en todo momento el movimiento del robot y responder ante situaciones imprevistas.

\section{Infraestructura desarrollada}
En este apartado se introducirán los distintos componentes sobre los que se apoya la práctica, principalmente del hardware empleado y sus drivers controladores (Figura 4.1).

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth, height=5.5cm]{figures/disposicionff.png}
		\caption{Infraestructura de Follow Face}
		\label{fig.dispff}
		\end{center}
\end{figure}

\subsection{Cámara Sony Evi d100p}
El robot utilizado en esta práctica será la cámara Sony Evi modelo d100p, como el que se muestra en la Figura inferior (Figura 4.2), de la multinacional japonesa especializada en electrónica, \textit{gaming} y entretenimiento.

\begin{figure}[h]
	\centering
	\begin{minipage}[h]{.48\linewidth}
		\centering
		\includegraphics[width=.9\linewidth, height=6cm]{figures/sonyevi.png}
		\captionof{figure}{Cámara Sony Evi d1oop}
		\label{fig:sonyevi}
	\end{minipage}
	\begin{minipage}[h]{.48\linewidth}
		\centering
		\includegraphics[width=.9\linewidth, height=5cm]{figures/capturadora.jpg}
		\captionof{figure}{Digitalizador Dazzle DVD Recorder}
		\label{fig:digitalizadora}
	\end{minipage}
\end{figure}

\vspace{4.0cm}
Este modelo cuenta con las siguientes características:
\begin{itemize}
	\item[--] 440,000 pixeles que permiten tomas en alta resolución.
	\item[--] Además de la acción de Pan y Tilt de alta velocidad, la mejora del mecanismo de reducción de ruido para vídeo en color. 
	\item[--] Posibilidad de operación desde computador a través del protocolo VISCA.
	\item[--] Buffer de mensajes que permite memorizar hasta 6 combinaciones de la posición y el estado de la cámara. 
	\item[--] Control remoto multifunción. 
	\item[--] Compatibilidad de comandos con modelos anteriores.
\end{itemize}

Dado que el hardware captura vídeo analógico, en combinación con la cámara se ha usado una capturadora de vídeo, que es un periférico de entrada cuya función es recibir señales de audio/video procedentes de dispositivos analógicos y convertirlas en señales digitales que puedan ser almacenadas o manipuladas por medio de software.

La capturadora utilizada es Dazzle DVD Recorder HD de la marca Pinnacle (Figura 4.3), y con conexión USB 2.0 con el ordenador. En cuanto a las capacidades del hardware en conjunto, nos interesan:

El valor de \textit{Pan} puede estar entre -164º y 164º y su velocidad puede tomar valores en un rango donde 1 es el mínimo y 24 es el máximo. El valor de \textit{Tilt} puede oscilar entre -30º y  30º, con velocidades en una escala de 1 a 20.

La cámara utilizará \textit{drivers} para el acceso a la imagen y a los actuadores y para la comunicación con el nodo académico.

\subsection{Ficheros de configuración}
Al haber varios \textit{drivers} involucrados, son necesarios ciertos ficheros de configuración en el nodo académico para calibrar las conexiones con ellos. Dado que el primero de ellos es un driver de ROS (\textit{usb\_cam}), emplea la herramienta \textit{roslaunch}\footnote{\url{http://wiki.ros.org/roslaunch}}, que permite lanzar fácilmente múltiples nodos ROS de manera local o remota. Para ello necesita un fichero de configuración XML como el siguiente (Figura 4.4): 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/usbcamconf.jpg}
		\caption{Fichero de configuración de usb\_cam}
		\label{fig.usbcamconf}
		\end{center}
\end{figure}

En él deben especificarse los parámetros del nodo o nodos a establecer, así como los dispositivos sobre los cuales se lanza el nodo, en nuestro caso, la cámara (\textit{/dev/video1}).

El otro \textit{driver} involucrado (\textit{EVIcam}) ha de lanzarse también con un fichero de configuración que indique el par \textit{IP:puerto} en el que la cámara espera órdenes y el dispositivo. Un ejemplo se muestra en la Figura 4.5:

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/evicamconf.png}
		\caption{Fichero de configuración de evicam\_driver}
		\label{fig.evicamconf}
		\end{center}
\end{figure}
 
\section{Nodo Académico}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/nodoacademicoff.png}
		\caption{Nodo Académico de Follow Face}
		\label{fig.nodoff}
		\end{center}
\end{figure}

El componente académico de esta práctica (Figura 4.6) resuelve varias funcionalidades auxiliares que sirven de gran ayuda para poder abordarla:

\begin{enumerate}[label=\alph*)]
	\item Proporciona un HAL-API, API de la \textit{Hardware Abstraction Layer}, es decir, los sensores y
actuadores de la cámara en este caso, en forma de métodos simples (oculta el \textit{middleware} de comunicaciones con el hardware);  
	\item Ofrece una interfaz gráfica al usuario que le ayuda a depurar su código, y proporciona un GUI-API, un interfaz simple para que el estudiante visualice cosas interesantes en este ejercicio; 
	\item Incluye código auxiliar que no es el foco del algoritmo y que ayuda a programar la solución.
\end{enumerate}

El nodo deja todo preparado para que el estudiante sólo tenga que incluir su código retocando el método \textit{execute} en el fichero \textit{MyAlgorithm.py}, e ir realizando las pruebas pertinentes.

\subsection{Arquitectura software}
En cuanto a la capacidad de cómputo para realizar todas las tareas simultáneamente, el componente emplea dos hilos de ejecución para agilizar la respuesta de los distintos módulos:
\begin{itemize}
	\renewcommand{\labelitemi}{$\to$}
	\item Hilo de algoritmo de percepción y control: se encarga del refresco de la ejecución del algoritmo, ya que este se ejecuta de modo cíclico. El tiempo de refresco de este hilo es muy importante, dado que su velocidad permitirá reaccionar ante situaciones imprevistas, y adaptar el movimiento de la cámara a la velocidad de movimiento de las personas, para poder seguir su cara. Es por eso que el tiempo de refresco es de 80 ms.
	\item Hilo de la interfaz gráfica de usuario (GUI): Este hilo es el encargado de actualizar la interfaz gráfica. También consta de los manejadores de eventos del GUI. El intervalo de actualización de la interfaz debe ser pequeño, ya que se tiene que mostrar las imágenes captadas por la cámara en forma de elemento vídeo, así como las imágenes segmentados en tiempo real. Por ello, se ha fijado el tiempo de refresco a 50 ms.
\end{itemize}

\subsection{Interfaz de sensores y actuadores}
El componente ofrece un API de cámara y actuadores al programador con el cuál acceder a la funcionalidad que necesita:

\begin{itemize}
	\item \textit{self.camera.getImage()}: devuelve la imagen capturada por la cámara activa (local o cámara usb).
	\item \textit{self.motors.getLimits()}: método a través del cual se pueden obtener los límites de Pan y Tilt (en grados).
	\item \textit{self.motors.setPTMotorsData(pan, tilt, panSpeed, tiltSpeed)}: método para enviar comandos de \textit{Pan} y \textit{Tilt} a la cámara. Necesita los valores respectivos en grados y la velocidad horizontal y vertical para alcanzar la posición fijada.
\end{itemize}

\subsection{Interfaz gráfica y uso desde código}
La interfaz gráfica de usuario (GUI) de la práctica sirve para representar información importante para desarrollar correctamente el algoritmo que lleve a la solución, además de para proporcionar funcionalidad básica para iniciar o para el algoritmo o para teleoperar la cámara. Se ha programado en Python con PyQt5, y es la mostrada en la Figura 4.7.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/guifollowface.png}
		\caption{Interfaz Gráfica de Follow Face}
		\label{fig.guifollowface}
		\end{center}
\end{figure}

Se puede ver en ella, a la izquierda, el teleoperador para comprobar que el movimiento de la cámara este operativo. Se trata de un dial bidimensional que controla las componentes \textit{Pan} y \textit{Tilt} del robot. Únicamente será necesario mover el \textit{joystick} central en sentido vertical, para controlar el ángulo de \textit{Pan} y en sentido horizontal para fijar el \textit{Tilt}. Cuánto más se acerque el \textit{joystick} a los límites del teleoperador, más acentuado será el valor de la componente hacia la que se esté movimiento el mismo, ya sea éste un valor negativo si se mueve hacia abajo o hacia la izquierda, y positivo en caso de que se mueva hacia arriba o a la derecha. A su derecha, se han colocado varios botones cuya funcionalidad es la de iniciar el algoritmo, pararlo o resetearlo. Recordemos que la cámara dispone de un \textit{buffer} de mensajes, de manera que pulsar el botón Stop se traducirá en pausar el algoritmo, pero la cámara puede continuar en movimiento si había encolado mensajes, hasta que vacíe dicho \textit{buffer}. Inmediatamente debajo tenemos un \textit{check} que muestra la herramienta de imagen (Figura 4.8), en la cual se pueden ver las imágenes que provee la cámara (a la izquierda) y la imagen segmentada (a la derecha, en caso de haberla establecido):

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/imagetool.png}
		\caption{Visualización de imagen procesada en el GUI}
		\label{fig.imagetool}
		\end{center}
\end{figure}

Los métodos que pone a disposición del alumno son los siguientes:
\begin{itemize}
\item \textit{self.camera.setColorImage(img)}: método que recibe una imagen y la muestra en el espacio dedicado del interfaz para imágenes capturadas.
	\item \textit{self.camera.setThresholdImage(img)}: método que recibe una imagen tratada y la muestra en el espacio reservado en el GUI para imágenes filtradas, umbral o en blanco y negro.
\end{itemize}
Con ellos el estudiante podrá establecer imágenes en el GUI desde su código implementado.

\subsection{Fichero de configuración}
El nodo académico requiere un fichero de configuración que indique los \textit{endpoints} que utilizan la cámara y los motores PT. Este archivo tiene extensión \textit{.yml}, lo cual implica que debe seguir el formato de serialización de datos YAML, el cual produce datos fáciles de leer por humanos y máquinas para todos los lenguajes de programación. Tiene el siguiente aspecto:

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/ymlfollowface.png}
		\caption{Fichero de configuración YAML de FollowFace}
		\label{fig.followfaceconf}
		\end{center}
\end{figure}

Se puede ver claramente que las interfaces empleadas por cámara y motores son distintas, siendo en el primer caso ROS y en el segundo ICE. Se observa también que puede incluir información adicional, como el formato de imagen.

Por último, el nodo académico incluye dos ficheros de descripción XML, que servirán de entrenamiento para los clasificadores que necesitaremos en la segmentación, explicados con mayor detalle en el apartado 4.4.1.1. 

\subsection{Código auxiliar: Clase CameraSegment}
Dentro del nodo académico existe la clase \textit{CameraSegment}. Esta clase se ha creado por controlar y establecer la visualización de las imágenes que tienen que ver con la práctica, sustituyéndolas por imágenes negras si son nulas o estableciendo el vídeo capturado por la cámara y el vídeo segmentado creado y seleccionado por el programador.

El objeto \textit{self.camera} del cual dispone el algoritmo de percepción y control se construye a partir de ésta clase, por lo cual incluye algunos métodos propios además de los esperados por la cámara. Hablamos de las funciones que permiten mostrar las imágenes en el interfaz, descritas en 4.3.3. Por tanto, actúa de nexo entre la herramienta de segmentación (\textit{segment widget}) y el algoritmo, proporcionando un API fácil de utilizar.

Por último, esta clase actúa de intermediario entre la cámara y el interfaz, conectando ambos componentes. Esto es necesario dado que las imágenes capturadas por la cámara y las que espera el interfaz gráfico no son completamente compatibles: para poder ser visualizadas, las imágenes deben redimensionarse para encajar en la aplicación gráfica.

\section{Solución de referencia}
En esta sección, abordaremos brevemente los fundamentos de segmentación y detección de caras y el control reactivo gradual de hardware. Con ello construiremos una solución que se establecerá como solución de referencia, pero que sólo es uno de los muchos posibles métodos con el cual alcanzar el fin. El código quedará recogido en el fichero \textit{MyAlgorithm.py}, que es de naturaleza iterativa, de manera que en cada iteración se percibe, se actúa en consecuencia y se controla. Se implementa dentro del método \textit{execute}, el cual debe contener la lógica que se ejecuta de manera cíclica en el hilo de percepción y control.

\subsection{Detección de caras}
Las cámaras que incorporan los robots son una de las principales fuentes de información con las que cuentan, pero toda esta información es inútil sin la lógica que sea capaz de extraer la información relevante de la imagen. Algoritmos incapaces de diferenciar lo necesario de lo inane sólo conseguirán sobrecargar a la máquina de información inoperante, reduciendo su rendimiento e incluso llegando a producir comportamientos indeseados.

La detección de rostros se puede considerar como un caso específico de detección de “clases de objetos”. En la detección de clase de objeto, la tarea es encontrar las ubicaciones y tamaños de todos los objetos en una imagen que pertenecen a una clase determinada. Algunos ejemplos muy comunes en casos reales son torsos superiores, peatones y automóviles. De la misma forma, debemos ser capaces de encontrar caras en una imagen digital, sea cual sea su tamaño o ubicación en ella. Existen muchos algoritmos de detección de rostros para ubicar a una persona en una escena, algunos más fáciles y otros más complejos:

Cuando la imagen consta de un fondo monocolor simple o un fondo estático predefinido (conocido), es muy fácil detectar caras, dado que eliminar dicho fondo siempre tendrá como resultado los límites de la cara. Sin embargo, sabemos que en nuestro caso el robot debe comportarse correctamente en cualquier escenario, de manera que no es una solución válida.

Otra manera pasa por utilizar el color de piel típico para encontrar segmentos faciales, si se dispone de imágenes en color. La desventaja en este caso es que no funcionará con todo tipo de colores de piel, y que su robustez es mínima ante condiciones de iluminación variables. 

Aunque resulte sorprendente, la forma de abordar esta tarea con mayor éxito es utilizar imágenes en escala de grises, con lo cual reducimos el coste computacional. A su vez, hay varios métodos que las emplean, como son la detección basada en modelo, en emparejamiento de bordes por orientación, usando la distancia de Hausdorff\footnote{\url{https://en.wikipedia.org/wiki/Hausdorff_distance}},etc. Pero el más utilizado es mediante el uso de ``clasificadores en cascada'' encapsulado dentro de las técnicas de aprendizaje máquina que, utilizando características simples de Haar, puede producir resultados impresionantes.

Hay muchas motivaciones para usar características en la comparación en lugar de hacerlo con los píxeles directamente. La razón más común es que las características pueden codificar conocimiento del dominio \textit{ad-hoc}, lo cual es difícil usando una cantidad finita de datos de entrenamiento. Para este sistema también hay una segunda motivación crítica para emplear características: el sistema basado en características funciona mucho más rápido que un sistema basado en píxeles. 

Cada “ventana” de la Figura 4.10 se coloca en la imagen a segmentar para calcular una sola característica. Esta característica es un valor único que se obtiene al restar la suma de píxeles debajo de la parte blanca de la ventana de la suma de los píxeles debajo de la parte negra de la ventana. Así, todos los tamaños posibles de cada ventana se colocan en todas las ubicaciones posibles de cada imagen para calcular muchas características. Por ejemplo, en la imagen (Figura 4.10) estamos extrayendo dos características. La primera se centra en la propiedad de que la región de los ojos a menudo es más oscura que el área de la nariz y las mejillas. La segunda característica se basa en la propiedad de que los ojos son más oscuros que el puente de la nariz.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.69\linewidth]{figures/extraction.jpg}
		\caption{Extracción de características}
		\label{fig.extraction}
		\end{center}
\end{figure}

Entre todas estas características calculadas, la mayoría de ellas son irrelevantes. Por ejemplo, cuando se usa en la mejilla, las ventanas se vuelven irrelevantes porque ninguna de estas áreas es más oscura o más clara que otras regiones en las mejillas, todos los sectores en ella son iguales. Se debe descartar de inmediato las características irrelevantes y conservar únicamente aquellas importantes con una técnica llamada \textit{Adaboost} que selecciona sólo aquellas características conocidas para mejorar la precisión de clasificación (cara / no cara) del clasificador. Verifica si una ventana es una región sin rostro, y si no lo es, la desecha de inmediato para no volverla a procesar. Así el clasificador es capaz de enfocarse principalmente en el área donde exista una cara. Es por eso por lo que reciben el nombre de clasificadores en cascada (Figura 4.11), dado que se usan en primera instancia clasificadores simples para descartar esas áreas y luego clasificadores más complejos para eliminar falsos positivos.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.50\linewidth, height=4cm]{figures/cascade.jpg}
		\caption{Clasificadores en cascada}
		\label{fig.cascade}
		\end{center}
\end{figure}

\subsubsection{Clasificadores Máquina}
La manera escogida para abordar la tarea de detección en nuestra solución de referencia pasa por utilizar un clasificador en cascada. 
Estudiando la librería OpenCV para Python, hemos encontrado la manera de crear entrenadores y detectores, los cuales se pueden entrenar de la forma que se desee. Además la librería incluye sistemas pre-entrenados para detectar facciones concretas de la cara como ojos, sonrisa, cejas e incluso la cara en sí.

Estos sistemas de entrenamiento se encuentran en ficheros XML provistos por la librería, dos de los cuales se han incluido en el nodo académico de esta práctica para poder abordarla siguiendo este método propuesto por  Paul Viola y Michael Jones en su \textit{paper} \textit{``Rapid Object Detection using a Boosted Cascade of Simple Features''} en 2001. Estos ficheros son:

\textit{haarcascade\_frontalface\_default.xml} $\rightarrow$ entrenamiento para detectar caras en posición frontal.

\textit{haarcascade\_eye.xml} $\rightarrow$entrenamiento para clasificador de ojos.

Con ellos es suficiente para realizar una detección y segmentación genérica en condiciones normales. El alumno tiene la posibilidad de entrenar todos los clasificadores que quiera para su programa, simplemente incluyendo los ficheros \textit{.xml} necesarios en el directorio de trabajo. Una vez obtenidos, simplemente se invocará la función:

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.99\linewidth]{figures/classifiercode.png}
		\label{fig.classifiercode}
		\end{center}
\end{figure}
Para obtener un objeto Python con el clasificador deseado.

\subsubsection{Eliminación de falsos positivos}
Usar clasificadores en cascada es la mejor manera de reducir la tasa de falsos positivos en la detección. Sin embargo, bajo condiciones cambiantes de luminosidad o entornos con mucho contenido se tienen resultados etiquetados como positivos en casos que claramente no lo son (Figura 4.12):

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.70\linewidth]{figures/falsopositivo.jpg}
		\caption{Falso positivo}
		\label{fig.falsopositivo}
		\end{center}
\end{figure}

Proporcionar al sistema un entrenamiento más meticuloso sería la opción ideal para reducir aún más la tasa de error. Sin embargo, las prácticas en este entorno buscan que el alumno se enfrente únicamente a la lógica de control y obtención de datos de los robots, de manera que este problema debe abordarse con código de filtrado de los resultados.

Tras obtener la posición de todas las caras de la imagen con el método \textit{detectMultiScale} del objeto clasificador de OpenCV (en nuestro caso, \textit{face\_cascade}), aplicamos un filtrado que elimine positivos de tamaño inferior a un umbral mínimo, o superior a un umbral máximo para mejorar los resultados. Además, incluimos un clasificador para la detección de ojos que resultará más restrictivo a la hora de hacer una detección final. Finalmente, se utiliza la función \textit{rectangle} del paquete con las coordenadas de la cara para hacer visual la detección. Una vez aplicado el filtro, el resultado cambia (Figura 4.13):

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.70\linewidth]{figures/falsopositivosolved.jpg}
		\caption{Falso positivo solucionado}
		\label{fig.falsopositivosolved}
		\end{center}
\end{figure}

\subsection{Control del movimiento de la cámara}
En las cámaras PTZ estas siglas indican el control que se puede ejercer sobre la máquina, siendo que se puede modificar el valor que toma en \textit{Pan} (P), el que toma en \textit{Tilt} (T) y el que toma de \textit{zoom} (Z). Nuestra cámara no dispone de \textit{zoom}, se podría decir que consta de un control PT.

Ahora bien, tratándose de control en tiempo real, ¿puede tomar cada componente cualquier valor dentro de los límites del hardware? No, debemos contar con el tiempo físico que tarda la cámara en ejecutar el movimiento solicitado. También debe tenerse en cuenta el \textit{buffer} asociado, que encola mensajes hasta que puedan ser atendidos. Si se envía la misma orden dos veces antes de que el hardware sea capaz de terminar la primera (algo muy común), el resultado se traducirá en un “temblor” u oscilación de la cámara entorno a esos valores.

Es por eso que hemos hablado de control gradual. Se hace necesario que el movimiento del cuello mecánico sea a la vez fluido y suave, sin ejecutar movimientos demasiado bruscos, y sin oscilaciones que repercutan en la calidad de la imagen captada, para que el algoritmo de visión pueda seguir funcionando de manera óptima durante los movimientos.
Por ello, hemos decidido implementar un control que trabaje en incrementos o sustracciones pequeñas en grados de los valores \textit{Pan} y \textit{Tilt}. Con ello conseguimos también aprovechar la máxima velocidad de movimiento que ofrece el hardware, sin balanceos, obteniendo un movimiento adecuado en combinación con el tiempo de refresco del hilo de ejecución de algoritmo. En cada iteración se calcula el centro de la cara detectada y el control trabaja para que dicho centro coincida con el centro de la imagen captada. En caso de no ser así, se obtendrá el error de posición y se modificarán los valores de control de la cámara de la forma conveniente. En caso de coincidencia, se detendrán los motores y se hará que la cámara permanezca en espera al próximo movimiento.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.98\linewidth, height=17cm]{figures/snippet1ff.png}
		\label{fig.snippet1ff}
		\end{center}
\end{figure}

\subsection{Gestión de la ``No Detección''}
Hemos programado el comportamiento en caso de detección, la eliminación de falsos positivos, la lógica de control y la segmentación pero, ¿qué hace el programa propuesto en caso de no detectar caras en la imagen? Existen varios caminos posibles: el primero sería simplemente no hacer nada. El hardware permanece en espera a que una persona entre en escena. Se establece un módulo condicional en el código que detiene los motores en la posición actual si no detecta rostros. Otra opción sería hacer que el harware retornase a la posición de reposo, pero la solución por la que nos hemos decantado ha sido por iniciar un proceso de búsqueda de rostros dentro del “campo de visión” de la cámara. 

Cuando en la imagen no se encuentran secciones que casen con una cara, la cámara esperará 5 segundos como margen para seguir comprobando en las siguientes iteraciones si realmente no hay caras visibles, o si por el contrario se trataba de un error de clasificación. Pasados estos 5 segundos, la cámara comienza un movimiento horizontal (a lo largo del eje X) hacia la derecha, dado que este eje es en el que más probabilidad existe de encontrar una cara, no sólo por las características de las personas (no podemos desplazarnos en vertical, volar), sino también porque es el eje con mayor recorrido del hardware: 328º frente a 60º. Mientras se desplaza sigue ejecutando el algoritmo de detección, de manera que en caso de obtener un positivo, se continúa con la ejecución normal de seguimiento y, en caso contrario, se efectúa el movimiento hasta que se llegue al extremo. Una vez en el extremo, efectúa el mismo movimiento hacia el otro lado. La cámara permanecerá en movimiento en todo momento hasta la detección de una nueva cara.

\section{Experimentación}
En este apartado validaremos la funcionalidad descrita en los apartados anteriores a través de experimentos realizados para poner a prueba tanto la infraestructura de la práctica, como el nodo y la solución.

\subsection{Ejecución típica}
Se ha preparado un documento de texto (\textit{README.md}) que sirve de guía para que alumno no encuentre dificultades durante la ejecución y el testeo, que incluye también información del API de utilización. La forma de ejecutar la práctica es la siguiente:

\begin{enumerate}
	\item Empleamos un primer terminal para inicial el driver de ROS \textit{usb\_cam} (previamente habrá que asegurarse de haber instalado el paquete \textit{ros-kinetic-usb-cam packet}):
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=0.95\linewidth]{figures/ffcomando1.png}
			\label{fig.ffcomando1}
		\end{center}
	\end{figure}
	\item Utilizamos un segundo terminal para iniciar el driver \textit{evicam\_driver}:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=0.95\linewidth]{figures/ffcomando2.png}
			\label{fig.ffcomando2}
		\end{center}
	\end{figure}
	\item Por último, se lanza en otro terminal el nodo académico \textit{follow\_face}:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[width=0.95\linewidth]{figures/ffcomando3.png}
			\label{fig.ffcomando3}
		\end{center}
	\end{figure}
\end{enumerate}

El resultado de la ejecución de la solución canónica se muestra en la Figura 4.14. Además, dos vídeos demostrativos del comportamiento pueden verse en \footnote{\url{https://www.youtube.com/watch?v=pdSPnftumf8}} y \footnote{\url{https://www.youtube.com/watch?v=YZEMIXkXwM0}}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.98\linewidth]{figures/ffoutput.jpg}
		\caption{Output Follow Face}
		\label{fig.ffoutput}
		\end{center}
\end{figure}

La cámara persigue con éxito la cara segmentada en caso de haberla a través tanto de movimientos horizontales como verticales en función de las coordenas obtenidas en la imagen, ejecutando movimientos suaves y controlados. Cuando no detecta rostros comienza el proceso de búsqueda hasta encontrar una nueva cara.

\subsection{Comportamiento ante casos extremos}
Hemos tratado de poner el algoritmo a prueba para comprobar su validez. Una primera situación que debimos probar fue la ejecución con movimiento rápido. La persona al frente de la cámara se movió de manera brusca, cambiando de dirección y con subidas y bajadas periódicas. Gracias al movimiento gradual implementado, el algoritmo es capaz de seguir en todo momento el movimiento, salvo en aquellos casos en los que el sujeto se sale bruscamente del plano. 

Por otro lado, probamos la ejecución en situaciones de contraluz. En este caso, el comportamiento empeoraba, ya que la imagen captada, al convertirse a escala de grises, no mostraba apenas diferencia de niveles en píxeles que conforman secciones de la cara que normalmente si presentan dicha diferencia. Esto se debe a las bases de la detección con clasificadores en cascada. Se podría tratar de solucionar incorporando técnicas de detección de color de piel, entre otras.

Por último, en situaciones con muchos candidatos a caras (posters, muñecos y elementos con formas abstractas), el algoritmo se equivocaba el 5\% de las veces, siendo que en el resto de casos escogía el candidato correcto para seguir.

\section{Cuadernillo Jupyter Académico}
En el apartado 3.7 se introdujo la herramienta Jupyter Notebook,  que permite al usuario crear y compartir documentos que contengan multitud de elementos y widgets programables a través de distintos lenguajes. Hemos ampliado el alcance inicial de esta práctica preparando, además de la versión con nodo académico en python, una versión en un cuadernillo Jupyter. Esto permite que el estudiante no tenga que
editar el fichero de python directamente, sino utilizar el navegador web para ello. Esto abre puertas en JdeRobot-Academy hacia versiones multiplataforma de los ejercicios, especialmente los que involucran al simulador Gazebo.

Antes de comenzar, debe instalarse el código del proyecto, proceso del que se detallen las instrucciones en la página oficial del proyecto\footnote{\url{http://jupyter.org/install}}.

Hemos preparado un \textit{Notebook} de Jupyter al que hemos llamado \textit{follow\_face.ipynb}, que junto con algunos cambios en la estructura del nodo académico proporcionan el soporte necesario para tener la misma práctica disponible a través de esta aplicación. En concreto, el nodo programado en el fichero \textit{follow\_face.py} de la práctica original ha sido recodificado para tener estructura de clase Python: la clase \textit{FollowFace}, para poder ser instanciada desde el código del cuadernillo, que detallaremos un poco más adelante. Los cambios más significativos se producen al eliminar la interfaz gráfica directa, dado que será ahora la propia aplicación web la que actúe de interfaz de usuario. Eliminados los componentes gráficos y los widgets que mostraban las imágenes en él, se ha creado una nueva clase (la clase \textit{Printer}, Figura 4.15) que permite visualizar las imágenes que el alumno establece a través de métodos disponibles en la clase nodo (\textit{FollowFace}) en la interfaz de Jupyter. Para ello utilizamos los métodos disponibles en la librería \textit{matplotlib} y su módulo \textit{pyplot},  una colección de funciones cuyo estilo es como un comando que hacen que \textit{matplotlib} funcione como MATLAB en cuanto a representaciones, lo cual simplifica la visualización en la herramienta. 

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.98\linewidth]{figures/printer.jpg}
		\caption{Printer}
		\label{fig.printer}
		\end{center}
\end{figure}

Para asociar el código del cuadernillo con el del nodo existente se ha usado la librería \textit{types} de Python. Esta librería proporciona acceso a los nombres de algunos tipos de objetos que son utilizados por el intérprete estándar de Python, de manera que podemos modificar el comportamiento normal de los mismos. Utilizaremos la función \textit{MethodType} para hacer corresponder la función que el estudiante retoca desde la aplicación de Jupyter con el método que realiza el cómputo en el hilo de algoritmo del nodo académico modificado. 

Por último, necesitamos de la intervención de dos \textit{drivers} para controlar la cámara de la que disponemos, de manera que la adaptación del nodo incorpora el lanzamiento de dos subprocesos, uno para cada \textit{driver}, a través de la librería \textit{subprocess}, que permite generar nuevos procesos y conectarse a sus tuberías de entrada / salida / error, además de obtener sus códigos de retorno. Con esto, y unas pequeñas incorporaciones al nodo para poder parar o ejecutar el algoritmo desde Jupyter, completamos la práctica a través de la aplicación web. 

El cuadernillo generado (Figura 4.16) no sólo incorpora celdillas de código ejecutable embebidas, sino que también incluye imágenes representativas y texto descriptivo para orientar al alumno en todo momento (durante la ejecución, aportando información adicional para abordar la práctica y explicando el API de utilización con los nuevos métodos de visualización y actualización del código. 

Así, el alumno puede realizar la práctica de manera totalmente guiada, y ejecutar los distintos pasos que va programando para observar su salida con herramientas disponibles, para depurar errores o matizar su solución.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.98\linewidth]{figures/notebookff.jpg}
		\caption{Notebook Follow Face}
		\label{fig.notebook}
		\end{center}
\end{figure}

Un vídeo demostrando el comportamiento de la práctica a través de Jupyter se puede ver en \footnote{\url{https://www.youtube.com/watch?v=9fOqkw7fUHg}}.